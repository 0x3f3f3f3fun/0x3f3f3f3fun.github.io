<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xinqi Lin (ÊûóÂøÉÊ∑á)</title>

    <meta name="author" content="Xinqi Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Xinqi Lin (ÊûóÂøÉÊ∑á)</name>
                                    </p>
                                    <p>
                                        I am a second-year graduate student at
                                        <a href="https://xpixel.group/">XPixelGroup</a>, Shenzhen Institutes of
                                        Advanced Technology, Chinese Academy of Sciences.
                                        I am supervised by Prof. <a
                                            href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a>.
                                        I also work closely with Dr. <a href="https://www.jasongt.com/">Jinjin Gu</a>.
                                    </p>
                                    <p>
                                        Prior to that, I received my B.Eng. from the Tianjin University (<a
                                            href="https://www.tju.edu.cn/">TJU</a>) in 2023.
                                    </p>
                                    <p>
                                        My current research interest mainly lies in <span class="highlight">image
                                            restoration, image super-resolution</span>.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="data/CV_XinqiLin.pdf">CV</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=yj3thKsAAAAJ">Google
                                            Scholar</a>
                                        &nbsp/&nbsp
                                        <a href="https://github.com/0x3f3f3f3fun">Github</a>
                                        <br>
                                        <a href="xqlin0613@gmail.com">Email</a>: xqlin0613 [at] gmail [dot] com
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/Profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                                            src="images/Profile.jpg" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>News</heading>
                                    <p>
                                        <strong>[2024.07]</strong>&nbsp;&nbsp;&nbsp;&nbsp;One paper to appear in ECCV
                                        2024. See you in Milan.<br>
                                        <strong>[2023.09]</strong>&nbsp;&nbsp;&nbsp;&nbsp;I reached my first 1,000 stars
                                        on GitHub!<br>
                                        <strong>[2023.07]</strong>&nbsp;&nbsp;&nbsp;&nbsp;I graduate and receive my
                                        Bachelor's degree from Tianjin University.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        *: Equal Contribution, ‚Ä†: Corresponding Author
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

                        <tr>
                            <td style="padding:10px;width:40%;vertical-align:middle">
                                <img src='images/DiffBIR.jpg' style="width:100%">
                            </td>
                            <td style="padding:10px;width:60%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2308.15070">
                                    <papertitle>
                                        DiffBIR: Toward Blind Image Restoration with Generative Diffusion Prior
                                    </papertitle>
                                </a>
                                <br>
                                <strong>Xinqi Lin*</strong>, Jingwen He*, Ziyan Chen, Zhaoyang Lyu, Bo Dai, Fanghua Yu,
                                Wanli Ouyang, Yu Qiao, Chao Dong‚Ä†
                                <br>
                                European Conference on Computer Vision (<strong>ECCV</strong>), 2024
                                <br>
                                <a href="https://arxiv.org/abs/2308.15070">paper</a>
                                /
                                <a href="https://0x3f3f3f3fun.github.io/projects/diffbir/">project page</a>
                                /
                                <a href="https://github.com/XPixelGroup/DiffBIR">code</a>
                                <p>
                                    We present <em>DiffBIR</em>, a general restoration pipeline that could handle
                                    different blind image restoration tasks in a unified framework.
                                </p>
                            </td>
                        </tr>
                        <tr>
                            <td style="padding:10px;width:40%;vertical-align:middle">
                                <img src='images/HYPIR.jpg' style="width:100%">
                            </td>
                            <td style="padding:10px;width:60%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2507.20590">
                                    <papertitle>
                                        Harnessing Diffusion-Yielded Score Priors for Image Restoration
                                    </papertitle>
                                </a>
                                <br>
                                <strong>Xinqi Lin</strong>, Fanghua Yu, Jinfan Hu, Zhiyuan You, Wu Shi, Jimmy S. Ren,
                                Jinjin Gu‚Ä†, Chao Dong‚Ä†
                                <br>
                                arXiv, 2025
                                <br>
                                <a href="https://arxiv.org/abs/2507.20590">paper</a>
                                /
                                <a href="https://hypir.xpixel.group/">project page</a>
                                /
                                <a href="https://github.com/XPixelGroup/HYPIR">code</a>
                                <p>
                                    We propose a simple and effective method, <em>HYPIR</em>, to achieve a good balance
                                    between restoration quality, fidelity, and speed.
                                </p>
                            </td>
                        </tr>
                        <tr>
                            <td style="padding:10px;width:40%;vertical-align:middle">
                                <img src='images/FOS.jpg' style="width:100%">
                            </td>
                            <td style="padding:10px;width:60%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2404.19500">
                                    <papertitle>
                                        Towards Real-world Video Face Restoration: A New Benchmark
                                    </papertitle>
                                </a>
                                <br>
                                Ziyan Chen*, Jingwen He*, <strong>Xinqi Lin</strong>, Yu Qiao, Chao Dong‚Ä†
                                <br>
                                Computer Vision and Pattern Recognition Workshops (<strong>CVPRW</strong>), 2024
                                <br>
                                <a href="https://arxiv.org/abs/2404.19500">paper</a>
                                /
                                <a href="https://ziyannchen.github.io/projects/VFRxBenchmark/">project page</a>
                                /
                                <a href="https://github.com/ziyannchen/VFRxBenchmark">code</a>
                                <p>
                                    We introduced new real-world datasets named FOS with a taxonomy of "Full, Occluded,
                                    and Side" faces from mainly video frames to study the applicability of current
                                    methods on videos.
                                </p>
                            </td>
                        </tr>
                        <tr>
                            <td style="padding:10px;width:40%;vertical-align:middle">
                                <img src='images/VEnhancer.jpg' style="width:100%">
                            </td>
                            <td style="padding:10px;width:60%;vertical-align:middle">
                                <a href="https://arxiv.org/abs/2407.07667">
                                    <papertitle>
                                        VEnhancer: Generative Space-Time Enhancement for Video Generation
                                    </papertitle>
                                </a>
                                <br>
                                Jingwen He, Tianfan Xue, Dongyang Liu, <strong>Xinqi Lin</strong>, Peng Gao, Dahua Lin,
                                Yu Qiao, Wanli Ouyang‚Ä†, Ziwei Liu
                                <br>
                                arXiv, 2024
                                <br>
                                <a href="https://arxiv.org/abs/2407.07667">paper</a>
                                /
                                <a href="https://vchitect.github.io/VEnhancer-project/">project page</a>
                                /
                                <a href="https://github.com/Vchitect/VEnhancer">code</a>
                                <p>
                                    We present <em>VEnhancer</em>, a generative space-time enhancement framework that
                                    improves
                                    the existing text-to-video results by adding more details in spatial domain and
                                    synthetic detailed motion in temporal domain.
                                </p>
                            </td>
                        </tr>
                        <tr>
                            <td style="padding:10px;width:40%;vertical-align:middle">
                                <img src='images/AdaptBIR.jpg' style="width:100%">
                            </td>
                            <td style="padding:10px;width:60%;vertical-align:middle">
                                <a href="https://www.sciencedirect.com/science/article/pii/S0031320324004102">
                                    <papertitle>
                                        AdaptBIR: Adaptive Blind Image Restoration with latent diffusion prior for
                                        higher fidelity
                                    </papertitle>
                                </a>
                                <br>
                                Yingqi Liu, Jingwen He, Yihao Liu, <strong>Xinqi Lin</strong>, Fanghua Yu, Jinfan Hu, Yu
                                Qiao, Chao Dong‚Ä†
                                <br>
                                Pattern Recognition(<strong>PR</strong>), 2024
                                <br>
                                <a href="https://www.sciencedirect.com/science/article/pii/S0031320324004102">paper</a>
                                <p>
                                    <em>AdaptBIR</em> aims to help diffusion models get their footing in the low-level
                                    vision field, solving the pain point of insufficient fidelity.
                                </p>
                            </td>
                        </tr>
        </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
            <tr>
                <td>
                    <heading>Education</heading>
                </td>
            </tr>
        </tbody>
    </table>
    <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/UCAS.jpg" , width="80">
                </td>
                <td width="75%" valign="center">
                    M.Eng. @ University of Chinese Academy of Sciences
                    <br>
                    Sept. 2023 - Present
                    <br>
                    GPA: 3.6 / 4.0
                    <br>
                    Advisor: Prof. <a href="https://xpixel.group/2010/01/20/chaodong.html">Chao Dong</a>
                </td>
            </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/TJU.jpg" , width="80"></td>
                <td width="75%" valign="center">
                    B.Eng. @ Tianjin University
                    <br>
                    Sept. 2019 - Jun. 2023
                    <br>
                    GPA: 3.84 / 4.0
                    <br>
                    Advisor: Prof. <a href="https://sites.google.com/site/junjiechen08/">Junjie Chen</a>
                </td>
            </tr>

            <table
                style="width:33%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                    <script type='text/javascript' id='clustrmaps'
                        src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=1STUULBuvvkcU91CTePPxbN6eUqZhz9l5Q7wdwabKa0'></script>
                </tbody>
            </table>

            <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                    <tr>
                        <td style="padding:0px">
                            <br>
                            <p style="text-align:right;font-size:small;">
                                Template from <a href="https://github.com/jonbarron/jonbarron_website">JonBarron</a>
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
            </td>
            </tr>
    </table>
</body>

</html>